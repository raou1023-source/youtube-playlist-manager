"""èª¬æ˜æ–‡ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - å†ç”Ÿãƒªã‚¹ãƒˆã®èª¬æ˜æ–‡ã‚’è‡ªå‹•ç”Ÿæˆ"""

import urllib.request
import urllib.parse
import json
from dataclasses import dataclass
from typing import Optional
from datetime import datetime


# ========================================
# é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ãƒªãƒ³ã‚¯å®šç¾©
# ========================================

MUSIC_STREAMING_SERVICES = {
    "Amazon Music": "https://music.amazon.co.jp/",
    "Spotify": "https://open.spotify.com/",
    "Apple Music": "https://music.apple.com/",
    "YouTube Music": "https://music.youtube.com/",
}

VIDEO_STREAMING_SERVICES = {
    "Netflix": "https://www.netflix.com/",
    "Prime Video": "https://www.amazon.co.jp/Prime-Video/",
    "Disney+": "https://www.disneyplus.com/",
    "U-NEXT": "https://video.unext.jp/",
    "Hulu": "https://www.hulu.jp/",
}

# ã‚«ãƒ†ã‚´ãƒªã¨é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
CATEGORY_SERVICE_MAP = {
    # éŸ³æ¥½ç³»ã‚«ãƒ†ã‚´ãƒª
    "music": "music",
    # æ˜ ç”»ãƒ»æ˜ åƒç³»ã‚«ãƒ†ã‚´ãƒª
    "film": "video",
    "movies": "video",
    "entertainment": "video",
    "anime": "video",
    "documentary": "video",
    "drama": "video",
    "shows": "video",
    # ä¸¡æ–¹ï¼ˆéŸ³æ¥½ã¨æ˜ åƒã®è¤‡åˆã‚«ãƒ†ã‚´ãƒªï¼‰
    "trailers": "both",
}


@dataclass
class ArtistInfo:
    """ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±"""
    name: str
    summary: str = ""
    official_site: str = ""
    twitter: str = ""
    instagram: str = ""
    wikipedia_url: str = ""


class WikipediaAPI:
    """Wikipedia API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    BASE_URL = "https://ja.wikipedia.org/w/api.php"
    EN_BASE_URL = "https://en.wikipedia.org/w/api.php"

    @classmethod
    def search_artist(cls, name: str, lang: str = "ja") -> Optional[ArtistInfo]:
        """ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±ã‚’Wikipediaã‹ã‚‰æ¤œç´¢

        Args:
            name: ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå
            lang: è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆ"ja" or "en"ï¼‰

        Returns:
            ArtistInfo or None
        """
        base_url = cls.BASE_URL if lang == "ja" else cls.EN_BASE_URL

        try:
            # ã¾ãšæ¤œç´¢ã—ã¦ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            search_params = {
                "action": "query",
                "list": "search",
                "srsearch": name,
                "srlimit": 1,
                "format": "json",
                "utf8": 1,
            }
            search_url = f"{base_url}?{urllib.parse.urlencode(search_params)}"

            with urllib.request.urlopen(search_url, timeout=10) as response:
                search_data = json.loads(response.read().decode("utf-8"))

            search_results = search_data.get("query", {}).get("search", [])
            if not search_results:
                # æ—¥æœ¬èªã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è‹±èªã§å†æ¤œç´¢
                if lang == "ja":
                    return cls.search_artist(name, lang="en")
                return None

            page_title = search_results[0]["title"]

            # ãƒšãƒ¼ã‚¸ã®æ¦‚è¦ã‚’å–å¾—
            extract_params = {
                "action": "query",
                "titles": page_title,
                "prop": "extracts|info",
                "exintro": True,
                "explaintext": True,
                "exsentences": 3,
                "inprop": "url",
                "format": "json",
                "utf8": 1,
            }
            extract_url = f"{base_url}?{urllib.parse.urlencode(extract_params)}"

            with urllib.request.urlopen(extract_url, timeout=10) as response:
                extract_data = json.loads(response.read().decode("utf-8"))

            pages = extract_data.get("query", {}).get("pages", {})
            if not pages:
                return None

            page = list(pages.values())[0]
            summary = page.get("extract", "")
            wikipedia_url = page.get("fullurl", "")

            # å¤–éƒ¨ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆã€SNSãªã©ï¼‰
            extlinks_params = {
                "action": "query",
                "titles": page_title,
                "prop": "extlinks",
                "ellimit": 50,
                "format": "json",
                "utf8": 1,
            }
            extlinks_url = f"{base_url}?{urllib.parse.urlencode(extlinks_params)}"

            official_site = ""
            twitter = ""
            instagram = ""

            try:
                with urllib.request.urlopen(extlinks_url, timeout=10) as response:
                    extlinks_data = json.loads(response.read().decode("utf-8"))

                pages = extlinks_data.get("query", {}).get("pages", {})
                if pages:
                    page = list(pages.values())[0]
                    extlinks = page.get("extlinks", [])

                    for link_obj in extlinks:
                        link = link_obj.get("*", "")
                        if "twitter.com" in link or "x.com" in link:
                            if not twitter:
                                twitter = link
                        elif "instagram.com" in link:
                            if not instagram:
                                instagram = link
                        elif ("official" in link.lower() or
                              "å…¬å¼" in link or
                              name.lower().replace(" ", "") in link.lower()):
                            if not official_site:
                                official_site = link
            except Exception:
                pass  # å¤–éƒ¨ãƒªãƒ³ã‚¯å–å¾—å¤±æ•—ã¯ç„¡è¦–

            return ArtistInfo(
                name=name,
                summary=summary,
                official_site=official_site,
                twitter=twitter,
                instagram=instagram,
                wikipedia_url=wikipedia_url,
            )

        except Exception as e:
            print(f"Wikipedia API ã‚¨ãƒ©ãƒ¼: {e}")
            return None


class DescriptionGenerator:
    """å†ç”Ÿãƒªã‚¹ãƒˆèª¬æ˜æ–‡ç”Ÿæˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.wikipedia = WikipediaAPI()

    def generate_streaming_links(self, category: str) -> str:
        """é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ

        Args:
            category: ã‚«ãƒ†ã‚´ãƒªå

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸé…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ãƒªãƒ³ã‚¯æ–‡å­—åˆ—
        """
        service_type = CATEGORY_SERVICE_MAP.get(category.lower(), "music")
        lines = []

        if service_type in ("music", "both"):
            lines.append("ğŸ“€ éŸ³æ¥½é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹:")
            for name, url in MUSIC_STREAMING_SERVICES.items():
                lines.append(f"ãƒ»{name}: {url}")
            lines.append("")

        if service_type in ("video", "both"):
            lines.append("ğŸ¬ å‹•ç”»é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹:")
            for name, url in VIDEO_STREAMING_SERVICES.items():
                lines.append(f"ãƒ»{name}: {url}")
            lines.append("")

        return "\n".join(lines)

    def generate_artist_info(self, artist_name: str) -> str:
        """ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±ã‚’ç”Ÿæˆ

        Args:
            artist_name: ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±æ–‡å­—åˆ—
        """
        info = self.wikipedia.search_artist(artist_name)
        if not info:
            return ""

        lines = ["ğŸ‘¤ ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±:"]

        if info.summary:
            # æ¦‚è¦ã‚’é©åº¦ãªé•·ã•ã«åˆ¶é™
            summary = info.summary[:300]
            if len(info.summary) > 300:
                summary += "..."
            lines.append(summary)
            lines.append("")

        # å…¬å¼ãƒªãƒ³ã‚¯
        has_links = info.official_site or info.twitter or info.instagram or info.wikipedia_url
        if has_links:
            lines.append("ğŸ”— å…¬å¼ãƒªãƒ³ã‚¯:")
            if info.official_site:
                lines.append(f"ãƒ»å…¬å¼ã‚µã‚¤ãƒˆ: {info.official_site}")
            if info.twitter:
                lines.append(f"ãƒ»Twitter: {info.twitter}")
            if info.instagram:
                lines.append(f"ãƒ»Instagram: {info.instagram}")
            if info.wikipedia_url:
                lines.append(f"ãƒ»Wikipedia: {info.wikipedia_url}")
            lines.append("")

        return "\n".join(lines)

    def is_single_keyword_search(self, keywords: list[str], additional: str) -> tuple[bool, str]:
        """å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‹ã©ã†ã‹ã‚’åˆ¤å®š

        Args:
            keywords: é¸æŠã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
            additional: è¿½åŠ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

        Returns:
            (å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã©ã†ã‹, ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå)
        """
        all_keywords = keywords.copy()
        if additional:
            all_keywords.extend(additional.split())

        # 1ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã®å ´åˆã¯ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåã¨ã—ã¦æ‰±ã†
        if len(all_keywords) == 1:
            return True, all_keywords[0]

        # è¿½åŠ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ãŒå…¥åŠ›ã•ã‚Œã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãªã„å ´åˆ
        if not keywords and additional and " " not in additional.strip():
            return True, additional.strip()

        return False, ""

    def generate_description(
        self,
        era: str,
        category: str,
        keywords: list[str],
        additional_keyword: str,
        country: str,
        video_count: int,
        add_detailed: bool = True,
    ) -> str:
        """å®Œå…¨ãªèª¬æ˜æ–‡ã‚’ç”Ÿæˆ

        Args:
            era: å¹´ä»£
            category: ã‚«ãƒ†ã‚´ãƒª
            keywords: é¸æŠã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            additional_keyword: è¿½åŠ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            country: å›½/åœ°åŸŸ
            video_count: å‹•ç”»æ•°
            add_detailed: è©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ ã™ã‚‹ã‹

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡
        """
        lines = []

        # åŸºæœ¬æƒ…å ±
        all_keywords = keywords.copy()
        if additional_keyword:
            all_keywords.append(additional_keyword)
        keyword_str = ", ".join(all_keywords) if all_keywords else "ãªã—"

        lines.append(f"ğŸ“‹ å†ç”Ÿãƒªã‚¹ãƒˆæƒ…å ±")
        lines.append(f"ãƒ»å¹´ä»£: {era}")
        lines.append(f"ãƒ»ã‚«ãƒ†ã‚´ãƒª: {category}")
        lines.append(f"ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword_str}")
        if country != "å…¨ä¸–ç•Œ":
            lines.append(f"ãƒ»åœ°åŸŸ: {country}")
        lines.append(f"ãƒ»å‹•ç”»æ•°: {video_count}æœ¬")
        lines.append(f"ãƒ»ä½œæˆæ—¥: {datetime.now().strftime('%Y/%m/%d')}")
        lines.append("")

        if not add_detailed:
            return "\n".join(lines)

        # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±ï¼ˆå˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å ´åˆï¼‰
        is_single, artist_name = self.is_single_keyword_search(keywords, additional_keyword)
        if is_single and artist_name:
            artist_info = self.generate_artist_info(artist_name)
            if artist_info:
                lines.append(artist_info)

        # é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ãƒªãƒ³ã‚¯
        streaming_links = self.generate_streaming_links(category)
        if streaming_links:
            lines.append(streaming_links)

        return "\n".join(lines)

    def generate_simple_description(
        self,
        era: str,
        category: str,
        video_count: int,
        country: str = "å…¨ä¸–ç•Œ",
    ) -> str:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªèª¬æ˜æ–‡ã‚’ç”Ÿæˆ

        Args:
            era: å¹´ä»£
            category: ã‚«ãƒ†ã‚´ãƒª
            video_count: å‹•ç”»æ•°
            country: å›½/åœ°åŸŸ

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜æ–‡
        """
        desc = f"ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã•ã‚ŒãŸ{era}ã®{category}å‹•ç”» ({video_count}æœ¬)"
        if country != "å…¨ä¸–ç•Œ":
            desc += f" - åœ°åŸŸ: {country}"
        return desc


# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç›´æ¥å®Ÿè¡Œæ™‚ã®ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    generator = DescriptionGenerator()

    # ãƒ†ã‚¹ãƒˆ: é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ãƒªãƒ³ã‚¯
    print("=== éŸ³æ¥½ã‚«ãƒ†ã‚´ãƒª ===")
    print(generator.generate_streaming_links("music"))

    print("=== æ˜ ç”»ã‚«ãƒ†ã‚´ãƒª ===")
    print(generator.generate_streaming_links("film"))

    # ãƒ†ã‚¹ãƒˆ: ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±
    print("=== ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ± ===")
    print(generator.generate_artist_info("å®‡å¤šç”°ãƒ’ã‚«ãƒ«"))

    # ãƒ†ã‚¹ãƒˆ: å®Œå…¨ãªèª¬æ˜æ–‡
    print("=== å®Œå…¨ãªèª¬æ˜æ–‡ ===")
    print(generator.generate_description(
        era="2020s",
        category="music",
        keywords=["å®‡å¤šç”°ãƒ’ã‚«ãƒ«"],
        additional_keyword="",
        country="æ—¥æœ¬",
        video_count=20,
        add_detailed=True,
    ))
